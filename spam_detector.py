# -*- coding: utf-8 -*-
"""Spam_Detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-AZLk7UFi-WbYIAemDMQe4S6gZ6xNu4
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/spam.csv', encoding='latin-1')
df.head()

"""Data Cleaning"""

#checking shape of data
df.shape

df.info()

#dropping unnesscary cols
df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True, errors='ignore')
df.head()

#renaming cols
df.rename(columns={'v1': 'Target', 'v2': 'Text'},inplace = True)
df.head()

#encoding col
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df['Target'] = encoder.fit_transform(df['Target'])
df.head()
#0=ham , 1=spam

#checking for missing values
df.isnull().sum()

#checking for duplicates
df.duplicated().sum()

df = df.drop_duplicates(keep = 'first')

df.duplicated().sum()

df.shape

"""EDA"""

# Counting the number of samples in each class of the target variable
# This helps understand class distribution (spam vs ham)
df['Target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['Target'].value_counts(),labels = ['ham','spam'],autopct='%0.2f')
plt.savefig("target_pie.png")
plt.show()

#there is imbalance in data

import nltk
#used to split text into words and sentences
nltk.download('punkt')
#Download additional Punkt resources required by some environments
nltk.download('punkt_tab')
#Used to remove common words like "the", "is", "and" during text preprocessing
nltk.download('stopwords')

#create a new feature that stores the number of characters in SMS
#Captures message length and formatting (spam often noisy)
df['num_characters'] = df['Text'].apply(len)
df.head()

#Creating a new feature that counts the number of words in each SMS
#Captures content size and verbosity
#The text is split by spaces and the total number of words is calculated
df['num_words'] = df['Text'].apply(lambda x: len(x.split()))
df.head()

#Create a feature representing the total word count of each message
#Captures message structure and intent
df['num_sentences'] = df['Text'].fillna('').apply(lambda x: len([s for s in x.split('.') if s.strip() != '']))
df.head()

#Statistical summary of features
df[['num_characters','num_words','num_sentences']].describe()

'''
Count->the number of values(after cleaning)
Mean->on average message have 79 characters, 15 words, 2 sentences
STD->the spread of the data (some messages are much longer pr shorter than the average)
Min/Max->for shortest 2 characters,1 sentence, 1 word and for longest 910 characters, 171 words,30 sentences
25%, 50%, 75% (quartiles)->Shows distribution of message length: most messages are short, but there are occasional very long spam messages
'''

#ham
df[df['Target'] == 0][['num_characters','num_words','num_sentences']].describe()

#spam
df[df['Target'] == 1][['num_characters','num_words','num_sentences']].describe()

'''
Insights:
Spam messages often lie in the higher range of characters, words, or sentences
These numeric features can help classifiers distinguish spam vs ham alongside text content
'''

import seaborn as sns
sns.histplot(df[df['Target'] == 0]['num_characters'])
sns.histplot(df[df['Target'] == 1]['num_characters'], color = 'red')
plt.savefig("num_characters_hist.png")

# Select only numeric columns
numeric_df = df.select_dtypes(include='number')


sns.heatmap(numeric_df.corr(), annot=True, cmap='YlGnBu')
plt.savefig("correlation_heatmap.png")
plt.show()

#Features are correlated

"""Text Preprocessing"""

#steps in data preprocessing
#--lower case
#--tokenization
#--removing special characters
#--removing stop words and punctuation
#--steming

import nltk
from nltk.corpus import stopwords
import string
from nltk.stem.porter import PorterStemmer
def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)


    y = []
    for i in text:
      if i.isalnum():
        y.append(i)

    text = y[:]
    y.clear()

    for i in text:
        if i not in stopwords.words('english') and i not in string.punctuation:
          y.append(i)


    text = y[:]
    y.clear()
    ps = PorterStemmer()
    for i in text:
        y.append(ps.stem(i))


    return " ".join(y)

transform_text('Hi How are you amna?')

transform_text('I love Yt lectures on machine learning,how about you?')

df['Text'][2]

df['transformed_text'] = df['Text'].apply(transform_text)

df.head()

#Shows which words appear most
from wordcloud import WordCloud
wc = WordCloud(width = 500,height = 500,min_font_size = 10,background_color = 'white')
spam_wc = wc.generate(df[df['Target']==1]['transformed_text'].str.cat(sep=" "))

plt.figure(figsize=(12,6))
plt.imshow(spam_wc)

ham_wc = wc.generate(df[df['Target']==0]['transformed_text'].str.cat(sep=" "))
plt.figure(figsize=(12,6))
plt.imshow(spam_wc)

# Building a corpus (list) of all words from selected messages
# - Iterate through all preprocessed messages of a particular class (spam or ham)
# - Split each message into words and append them to the list
# - The resulting list contains every word from the selected messages

spam_corpus = []
for msg in df[df['Target'] == 1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

len(spam_corpus)

from collections import Counter
import seaborn as sns
df_common = pd.DataFrame(Counter(spam_corpus).most_common(30))

sns.barplot(x=df_common[0], y=df_common[1])
plt.xticks(rotation='vertical')
plt.savefig("top_spam_words.png")
plt.show()

ham_corpus = []
for msg in df[df['Target'] == 0]['transformed_text'].tolist():
    for word in msg.split():
        ham_corpus.append(word)
len(spam_corpus)

df_common = pd.DataFrame(Counter(ham_corpus).most_common(30))
sns.barplot(x=df_common.iloc[:, 0], y=df_common.iloc[:, 1])
plt.xticks(rotation='vertical')
plt.savefig("top_ham_words.png")
plt.show()

"""Model Building"""

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
#Initialize CountVectorizer (converts text to a bag-of-words numeric matrix)
cv = CountVectorizer()
#Initialize TfidfVectorizer (converts text to TF-IDF weighted matrix)
tfidf = TfidfVectorizer()

x = cv.fit_transform(df['transformed_text']).toarray()
x.shape

y = df['Target'].values
y

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

#Initialize three types of Naive Bayes classifiers for text classification:

gnb = GaussianNB()      # Gaussian Naive Bayes, assumes continuous features follow a normal distribution
mnb = MultinomialNB()   # Multinomial Naive Bayes, works well for discrete count features like word counts
bnb = BernoulliNB()     # Bernoulli Naive Bayes, works with binary/boolean features (e.g., word present or not)

gnb.fit(x_train,y_train)
y_pred1 = gnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(x_train,y_train)
y_pred1 = mnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

bnb.fit(x_train,y_train)
y_pred1 = bnb.predict(x_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

#testing few other models too
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier

#Initialize a variety of machine learning classifiers for text classification
svc = SVC(kernel='sigmoid', gamma=1.0)            # Support Vector Classifier; good for non-linear decision boundaries
knc = KNeighborsClassifier()                       # K-Nearest Neighbors; classifies based on closest training samples
dtc = DecisionTreeClassifier(max_depth=5)         # Decision Tree; splits data on features, depth limited to avoid overfitting
lrc = LogisticRegression(solver='liblinear', penalty='l1')  # Logistic Regression; linear classifier, L1 regularization for feature selection
abc = AdaBoostClassifier(n_estimators=50, random_state=2)   # Adaptive Boosting; combines weak learners sequentially to improve performance
bc = BaggingClassifier(n_estimators=50, random_state=2)     # Bagging; ensemble of classifiers trained on random subsets of data
gbdt = GradientBoostingClassifier(n_estimators=50, random_state=2) # Gradient Boosting; sequential ensemble optimizing residual errors
xgb = XGBClassifier(n_estimators=50, random_state=2)         # Extreme Gradient Boosting; efficient gradient boosting, often high performance on structured data

models = {
    'SVC': svc,
    'KN' : knc,
    'DT' : dtc,
    'LR' : lrc,
    'ABC': abc,
    'BC' : bc,
    'GBDT': gbdt,
    'xgb': xgb

}

#Function to train a classifier and evaluate its performance
def train_classifier(clf, x_train, y_train, x_test, y_test):
    #Train the classifier on the training data
    clf.fit(x_train, y_train)

    #Predict labels for the test set
    y_pred = clf.predict(x_test)

    #Calculate accuracy and precision
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)

    #Return the performance metrics
    return accuracy, precision

train_classifier(SVC(kernel='sigmoid', gamma=1.0), x_train, y_train, x_test, y_test)

#Train and evaluate all models in the models dictionary
accuracy_scores = []
precision_scores = []
for name,model in models.items():
  current_accuracy,current_precision = train_classifier(model,x_train,y_train,x_test,y_test)
  print('for',name)
  print('accuracy',current_accuracy)
  print('precision', current_precision)

  #store metrics for comparision
  accuracy_scores.append(current_accuracy)
  precision_scores.append(current_precision)

#Create a DataFrame to summarize a model performances
#Sorted by Accuracy in descending order
performance_df = pd.DataFrame({
    'Algorithms': list(models.keys()),
    'Accuracy': accuracy_scores,
    'Precision': precision_scores
}).sort_values(by='Accuracy', ascending=False)

performance_df

performance_df.plot(
    x='Algorithms',
    y=['Accuracy', 'Precision'],
    kind='bar',
    figsize=(10, 5)
)

plt.ylim(0.5, 1.0)
plt.ylabel('Score')
plt.xticks(rotation=90)
plt.legend(title='Metric')
plt.tight_layout()
plt.savefig("algorithm_performance.png")
plt.show()

import pickle

pickle.dump(mnb, open("model.pkl", "wb"))
pickle.dump(tfidf, open("vectorizer.pkl", "wb"))